{"meta":{"title":"Moyu's blog","subtitle":null,"description":null,"author":"Moyu","url":"http://yoursite.com","root":"/"},"pages":[{"title":"相册","date":"2019-05-24T02:45:52.000Z","updated":"2019-05-24T02:45:52.000Z","comments":true,"path":"photo/index.html","permalink":"http://yoursite.com/photo/index.html","excerpt":"","text":"2016/03/27You Are Unique."}],"posts":[{"title":"如何加密磁盘（适用于全平台）","slug":"如何加密磁盘_VeraCrypt(全平台)","date":"2019-05-23T10:04:10.000Z","updated":"2019-05-24T02:50:02.000Z","comments":true,"path":"2019/05/23/如何加密磁盘_VeraCrypt(全平台)/","link":"","permalink":"http://yoursite.com/2019/05/23/如何加密磁盘_VeraCrypt(全平台)/","excerpt":"如何加密磁盘（适用于全平台）加密效果加密完成后的磁盘（硬盘）在插入任何系统之后均为“未挂载”状态，只有通过软件进行解密之后才会挂载成功，并可以显示使用。软件同时支持Windows,Mac OSX,Linux操作系统。 加密软件VeraCrypt:VeraCrypt官网","text":"如何加密磁盘（适用于全平台）加密效果加密完成后的磁盘（硬盘）在插入任何系统之后均为“未挂载”状态，只有通过软件进行解密之后才会挂载成功，并可以显示使用。软件同时支持Windows,Mac OSX,Linux操作系统。 加密软件VeraCrypt:VeraCrypt官网 优点 支持全平台 一次加密之后，可以同时在Mac OSX,Windows,Linux上使用。对于平时只用一个操作系统的小伙伴，如果是Mac，我推荐自带的加密软件；如果是只用Windows的小伙伴，我推荐bitlocker。 Mac加密/Bitlocker使用教程 只有平时可能同时在用多个操作系统的小伙伴更适合使用VeraCrypt对磁盘进行加密。 安全系数高 由TrueCrypt改进而来，是我目前知道的安全系数最高的加密软件。据说FBI也没有破开。 缺点 加密解密速度慢 毕竟安全系数最高，慢也是情有可原。不过确实好慢，我在U盘测试，16/32G U盘加密需要0.5小时左右，解密1.5小时左右。同学1T硬盘加密4小时左右。 需要安装软件 VeraCrypt需要提前在电脑上将客户端安装好才可以挂载加密磁盘，否则是无法挂载的。在这个上面是比较麻烦的。不如Mac加密和Bitlocker。 挂载加密 VeraCrypt是采用挂载加密，所以当你插入磁盘的时候，并不会挂载上，而这就会导致电脑弹出错误对话框。 Mac： 解决方法：只要不按“退出”就可以 Windows: 解决方法：只要不按“格式化磁盘”就可以： 教程如何已经了解了上述的各种优缺点之后仍然希望使用VeraCrypt软件，那么我们接下来进入教学篇。 安装教程 1.如果是Mac OSX系统，则需要首先下载安装FUSE FUSE下载网站 下载完成后打开，双击中间的安装包 傻瓜式安装中间需要注意这里： 2.下载安装VeraCrypt VeraCrypt下载网站 下载对应的操作系统版本，傻瓜式安装没什么好注意的。 使用教程Mac系统加密Mac系统用户操作比较复杂，因为全是英文版，我们就来细致的讲一下。 1、插入U盘2、在软件创建加密空间 3、根据提示操作 根据提示选择你的磁盘，如果要输入密码就输入你电脑的密码。 选择硬盘时候选择下面“Mount Dictionary”名字正确的那一个。点击下一步，弹窗全部选择“Yes”。 输入密码。 选择可能储存的文件类型 选择“exFAT” 选择上面的 到这一步，尽可能多动鼠标，随便晃，直到蓝条读满 最后就一路点下去，等待加密完成就可以了。 Mac系统挂载磁盘加密了之后就是如何挂载了。 1、插入加密后的磁盘这里就像上面缺点里面提到的，插入磁盘就会跳出“挂在失败“提示框。忽略掉就可以了。然后打开VeraCrypt软件。 2、在软件里找到磁盘 这里如果要密码，就输入你电脑的密码，不是磁盘密码。 3、挂载磁盘 输入密码，挂载磁盘，如果密码正确，只需稍等几秒就挂载上了。 4、挂载成功打开磁盘 到这里我们就挂载成功啦！ Mac系统解密emmmm我没找到Mac OSX的解密在哪里，所以我用windows里面的解密功能。 Windows系统加密中文版很简单，基本参照上面Mac教程来就可以。 Windows系统挂载中文版很简单，基本参照上面Mac教程来就可以。 Windows系统解密在软件“加密卷工具（T）…”里面有“永久解密”。点击之后根据提示输入密码，即可解密。解密时间比较久。","categories":[],"tags":[{"name":"自学教程","slug":"自学教程","permalink":"http://yoursite.com/tags/自学教程/"}]},{"title":"《机器学习.绪论》读后感","slug":"《机器学习.绪论》读后感","date":"2016-04-07T02:15:30.000Z","updated":"2019-05-24T02:49:56.000Z","comments":true,"path":"2016/04/07/《机器学习.绪论》读后感/","link":"","permalink":"http://yoursite.com/2016/04/07/《机器学习.绪论》读后感/","excerpt":"导读这本书我很早之前就有所耳闻。因为这本书的作者是我非常仰慕的周志华老师再加上我本人对机器学习很感兴趣，所以我也试着买了一本来读。","text":"导读这本书我很早之前就有所耳闻。因为这本书的作者是我非常仰慕的周志华老师再加上我本人对机器学习很感兴趣，所以我也试着买了一本来读。 正文这本书读着读着，我渐渐开始问自己：我到底是为什么喜欢机器学习，我究竟想用他来做什么，还是只是“好事者”。仔细想想，之前确实有被其名字吸引的成分在，感觉是这是趋势，于是便同很多人一般趋之若鹜。但是从暑假小学期自己编写了可以自己走的2048到计算机博弈大赛再到前一阵的AlphaGo人机大战，我渐渐认为我实在喜欢这种与计算机打交道的感觉。看过像名侦探柯南剧场版《贝克街的亡灵》中的“茧”，也看过像《刀剑神域》里面的游戏那样，我希望并喜欢着去看到这样的存在。所以我目前给自己定下的小目标是运用机器学习的知识来试着编写一个小的智能算法。比如计算机博弈又或者是模式识别。进入正题，机器学习，在这本书的绪论里面，我见识到的是机器学习整个庞大的体系框架，从符号主义学习，再到知识期，再到连结主义等等；包括“机械学习”、“示教学习”、“类比学习”以及“归纳学习”的分类都着实让我有一种打开眼界的感觉。如同把自己这个井底之蛙带入天空领略风景。之前我狭义地认为机器学习大多是“神经网咯”“深度学习”，现在想想幸好没有拿出去乱说，否则真是要让人笑掉大牙。本书以如何挑选一个好西瓜引入机器学习的正题，在我现在的初步认为比较像是在这个包含这个西瓜各种外部属性的多维空间中将属于好西瓜的那部分空间找出来。而且这段空间应该是连续的，也就是说我感觉很大程度上，这应该是一片空间，而我们要做的就是将其边界找出来。这样以后再有新的属性，我们只需要将其定位在空间里面看看是不是属于好瓜的范围就可以判断有这样属性的瓜是不是好瓜了。现在也突然可以理解为什么说原始的神经网络无法解决XOR的问题了，毕竟对于裸的是神经网络来说，每个神经元都是在做梯度下降法的拟合（这里不知道可不可以用遗传算法呀？以我短浅的学识感觉神经网络应该是可以解决的），所以这样的神经网络就是在做线性的拟合。所以说无法解决xor这样的非线性问题。及时给如大量的a与b已经其结果a^b也难以训练出很好的效果。基本属于：数据集(data set)：记录的集合。示例(instance)：每条记录是关于一个事件或对象的描述。属性(attribute)或特征(feature)：反省事件或对象在某方面的表现或性质的事项。属性值(attribute value)：属性上的取值。属性空间(attribute space)、样本空间(sample sapce)或输入空间：属性张成的空间。特征向量(feature vector):根据属性定位其在属性空间中的点对应的坐标向量。学习(leaning)或训练(training)：从数据中学得模型的过程。训练数据(training data)：训练过程中使用的数据。训练样本(training sample)：训练数据终端没一个样本。训练集(training set)：训练样本组成的集合。假设(hypothesis)：学得模型对应了关于数据的某种潜。分类(classification)：若我们预测的是离散值，此类学习任务称为“分类”。回归(regression)：若我们预测的是连续值，此类学习任务称为“回归”。簇(cluster)：将训练集中的样本分成若干组，每个组称为“簇”。泛化(generalization)能力：学得模型是用于新样本的能力。归纳(induction)：从特殊到一般的“泛化”过程。演绎(deduction)：从一般到特殊的“特化”(specialization)。奥卡姆剃刀：若有多个假设与观察一致，则选最简单的那个。没有免费午餐定理：总误差与学习方法无关。绪论的学习让我对机器学习有了一个整体的认识。但是我目前还是觉得机器学习是以拟合为主的，不知道他怎么样来解决一些非线性问题。 问题1.不明白既然裸的神经网络不可以解决xor这样的非线性的问题，那为什么不加入一些非线性的神经元呢？比如就是输入之间的xor再乘上权值。2.不知道神经网络里面的每个神经元如果不使用梯度下降法而使用遗传算法或者退火，粒子群等智能算法的话可不可以？这样会不会更好，因为这样会收敛于全局最优。3.不知道间循序渐进式学习（先进行无监督学习在进行有监督学习，再进行有无监督学习，再进行有监督学习，每个样本可以进行多次训练）会不会比只有一次的无监督学习+有监督学习好一些。因为我感觉这样更加贴近人类的学习，同一知识反复学习不断吸取新的经验。","categories":[],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"http://yoursite.com/tags/machine-learning/"},{"name":"自学","slug":"自学","permalink":"http://yoursite.com/tags/自学/"},{"name":"读后感","slug":"读后感","permalink":"http://yoursite.com/tags/读后感/"}]},{"title":"第一篇写一下写博客简单的流程","slug":"怎么写博客","date":"2016-04-07T02:15:30.000Z","updated":"2019-05-24T02:51:30.000Z","comments":true,"path":"2016/04/07/怎么写博客/","link":"","permalink":"http://yoursite.com/2016/04/07/怎么写博客/","excerpt":"欢迎来到莫与的博客，第一篇记录了一下怎么写一篇博客，以方便之后写博客~","text":"欢迎来到莫与的博客，第一篇记录了一下怎么写一篇博客，以方便之后写博客~ #从配置说起下载安装Git与Node.js略过 1.安装hexo1npm install hexo -g #-g表示全局安装, npm默认为当前项目安装 2.Hexo使用命令:123hexo init &lt;folder&gt; #执行init命令初始化hexo到你指定的目录hexo generate #自动根据当前目录下文件,生成静态网页hexo server #运行本地服务 更改主题1. 安装1git clone https://github.com/litten/hexo-theme-yilia.git themes/yilia 2. 配置修改hexo根目录下的 _config.yml ： theme: yilia 3. 更新12cd themes/yiliagit pull 怎么建立标题？建立一个1级标题：1# 建立一个1级标题： 建立一个2级标题：1## 建立一个2级标题 建立一个3级标题：1### 建立一个3级标题 建立一个超链接：效果:More info: Writing 代码如下：1More info: [Writing](https://hexo.io/docs/writing.html) 建立目录：代码如下：12categories: 动漫 （写在头上）效果看题头 建立标签：ategories: 动漫 #文章分類目錄 可以省略代码如下：12345678tags: 自学记录 (写在头上）若多个标签，则：tags:- first- essay- picture效果看题头 写代码：代码如下：12先写三个“`”带一个空格，后面写语言类型例如C++再写三个“`” 注意！1写markdown的时候，用txt打开，一定要用UTF-8保存。否则中文乱码。 最后加一幅图： 代码如下：1![1](怎么写博客/1.jpg) #怎么加音乐：（具体功能百度htmlaudio标签）123&lt;audio id=\"audio\" autoplay=\"autoplay\"&gt; &lt;source src=\"http://qzone.haoduoge.com/music1/2015-04-23/1429774382.mp3\" type=\"audio/mp3\"&gt;&lt;/source&gt;&lt;/audio&gt; #怎么改字体加颜色：1&lt;font color=\"red\" face=\"宋体\"&gt;摘要：&lt;/font&gt; #怎么加空格：1&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;","categories":[],"tags":[{"name":"自学教程","slug":"自学教程","permalink":"http://yoursite.com/tags/自学教程/"},{"name":"博客","slug":"博客","permalink":"http://yoursite.com/tags/博客/"}]},{"title":"基于信息量对于信息记忆的研究","slug":"基于信息量对于信息记忆的研究","date":"2016-04-07T02:15:30.000Z","updated":"2016-04-01T02:37:36.000Z","comments":true,"path":"2016/04/07/基于信息量对于信息记忆的研究/","link":"","permalink":"http://yoursite.com/2016/04/07/基于信息量对于信息记忆的研究/","excerpt":"[原创，转载请附网址:http://dongshuyan.top] 摘要我们应该怎么样去背东西呢？怎么样背东西快呢？为什么有的东西好背有的东西不好背呢？其实我们需要背诵东西的难易程度取决于其信息量大小。背诵东西的难易程度与其信息量是成正比的。这样一来，我们就把背诵的问题转化为如何减小信源，即要背诵的东西的信息量的问题了。就可以系统地研究如何背东西更容易了。 关键词背诵；记忆；信息量；","text":"[原创，转载请附网址:http://dongshuyan.top] 摘要我们应该怎么样去背东西呢？怎么样背东西快呢？为什么有的东西好背有的东西不好背呢？其实我们需要背诵东西的难易程度取决于其信息量大小。背诵东西的难易程度与其信息量是成正比的。这样一来，我们就把背诵的问题转化为如何减小信源，即要背诵的东西的信息量的问题了。就可以系统地研究如何背东西更容易了。 关键词背诵；记忆；信息量； 正文我们先将背诵的内容分为两类：无失真背诵与限失真背诵。先来说一说无失真背诵。什么是无失真背诵呢？顾名思义，就是不可以任何出现偏差地背诵，例如背单词，古诗词等。以一首绝句为例，一首绝句一共 5*4=20 个字。3000个汉字大约可以覆盖 99%的汉字， 所以这里假设一共有 3000个汉字可选。那么这样看的一首诗概率为 3000-20，由信息量计算公式，可得其信息量约为 231bit。如果这样直接一个字一个字的背诵的话，无疑是最吃力的。那么我们因该怎么样减少他的信息量，即增大概率呢？据我所统计的方法一共有一下三种：1.在不增字符的前提下， 让每个字出现的概率分布不均，方差越大越好2.让前后文之前有关系3.减少字符但是由于是无失真背诵，字出现的概率不可变，这里先不看第一个。在无失真背诵时的精髓就在于：强加前后文之间的关系！比如黄鹂，因为是鹂所以前面是形如词，是颜色概率较大，这样“黄”这个字的范围就比 3000 小多了。然后因为前面是黄鹂，后面一定高跟动词，这样又增大了“鸣”出现的概率。我们也可以用一些平时的常识或一些客观事物来强加前后文的因果关系。这样一来，我们就把复杂的无失真背诵转化为了对前后文关系的添加了。所以在无失真背诵中，最常用的方法就是为前后文之间添加联系，以方便背诵。再来看一看限失真背诵， 这里同样是要增大里面字符出现的概率，我们可以用如上的三种方法：首先，我们可以去掉冗余的字，即减少字符 ；其次，我们我们可以将意思相近的字变成同一个字，如果是背读音的话，可以变为同一个音。即在不增字符的前提下，让字符概率分布不均。例如：天下雨，我喝水，就可以变为：天下水，我喝水，这样来记。最后再利用无失真背诵的方法来强加关系减少信息量。最后我们还可以强加联系，例如：因为天下雨，所以有水，所以我喝水。这样就将“天下雨”与“我喝水”建立起来了一种因果关系。使得由于“天下雨”导致“我喝水”的概率增大，从而减小后文的信息量。所以，为了减小背诵的难度，我们则可以对号入座从而达到高效背诵的效果。 参考文献[1]曹雪虹.信息论与编码[M].北京.清华大学","categories":[],"tags":[{"name":"随笔","slug":"随笔","permalink":"http://yoursite.com/tags/随笔/"},{"name":"原创","slug":"原创","permalink":"http://yoursite.com/tags/原创/"}]},{"title":"单调栈学习笔记","slug":"单调栈学习笔记","date":"2016-04-07T02:15:30.000Z","updated":"2016-03-31T13:19:52.000Z","comments":true,"path":"2016/04/07/单调栈学习笔记/","link":"","permalink":"http://yoursite.com/2016/04/07/单调栈学习笔记/","excerpt":"作用&nbsp;&nbsp;&nbsp;&nbsp;利用单调栈，我们可以以O(n)的复杂度快速求解在a[1]~a[n]中每一个元素向某一个方向上的最长递减（或非递减等）区间长度。","text":"作用&nbsp;&nbsp;&nbsp;&nbsp;利用单调栈，我们可以以O(n)的复杂度快速求解在a[1]~a[n]中每一个元素向某一个方向上的最长递减（或非递减等）区间长度。 实现&nbsp;&nbsp;&nbsp;&nbsp;a[i] :第i个元素值&nbsp;&nbsp;&nbsp;&nbsp;st :为单调队列&nbsp;&nbsp;&nbsp;&nbsp;L[i] :为对于第i个元素，其向某个方向的最长递减(非递减)元素的下标。&nbsp;&nbsp;&nbsp;&nbsp;下面以向左非递减为例：&nbsp;&nbsp;&nbsp;&nbsp;建立单调队列st，对于从1开始每个元素进栈（由于是向左所以从左向右遍历）：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;1） 若原来栈空，则直接进栈。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;L[i]=1(1表示对于i个元素，从他一直到第1个元素都是每递减的)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st[top++]=a[i];&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2） 若原来栈不空，且栈顶元素st[top]&gt;a[i]，则说明即使加入a[i],对于i来说，其向左依旧是非递减。所以直接进栈。（注意：若要非递减，则不可以带等号！若要严格递减，则带等号，可以手动试）&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;L[i]=st[top]+1;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st[top++]=a[i];&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3） 若原来栈不空，且栈顶元素st[top]&lt;=a[i]，则一直出栈到满足st[top]&gt;a[i]或者栈为空为止，因为这样才能保证对于a[i]，其要找的是向左不递减。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;while (top&gt;0 &amp;&amp; a[st[top-1]]&gt;=a[j]) top–;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;l[j]=top==0?1:(st[top-1]+1);&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;st[top++]=j; 具体代码int t=0; for (int j=1;j&lt;=n;j++) { while (t&gt;0 &amp;&amp; a [st[t-1]]&gt;=a [j]) t--; l[j]=t==0?1:(st[t-1]+1); st[t++]=j; } 例题HDU2870：&nbsp;&nbsp;&nbsp;&nbsp;题目地址: Largest Submatrix&nbsp;&nbsp;&nbsp;&nbsp;题目大意：有个字母矩阵，包含字母”a、b、c、w、x、y、z”，其中，w能变为”a、b”，x能变为”b、c”，y能变为”a、c”，z能变为”a、b、c”。问能构成的最大字母完全一样的子矩阵面积为多大？&nbsp;&nbsp;&nbsp;&nbsp;解题思路:将全部字符依次转化a, b, c, 再分别求出这三个矩阵的最大子矩阵即可.于是, 问题转化为求矩阵中最大的子矩阵了. 设置一个变量Num[][]记录位置的最大高度, Num[i][j]表示Matritx[i][j]位置上的最大高度。 这样, 只要枚举以各个Num[i][j]为矩阵最小高度, 分别向前后推进扩展矩阵, 如果Num[i][j + 1] &gt;= Num[i][j]则可以向前扩展, 同理Num[i][j - 1] &gt;= Num[i][j]则可以向后扩展, 用L[j], R[j]分别表示当前位置 j 向前向后扩展的最大位置 剩下单调队列求L[j]、R[j]。 代码#include &lt;cstdio&gt; int n,m,ans; char ch; char aa[3][1005][1005]={0}; int a[3][1005][1005]={0}; int l[1005]={0}; int r[1005]={0}; int st[1005]={0};//栈 int main() { while (scanf(\"%d%d\",&amp;m,&amp;n)!=EOF) { getchar(); ans=0; for (int i=1;i&lt;=m;i++) { for (int j=1;j&lt;=n;j++) { scanf(\"%c\",&amp;ch); aa[0][i][j]=ch; aa[1][i][j]=ch; aa[2][i][j]=ch; if (ch=='w' || ch=='y' || ch=='z') aa[0][i][j]='a'; if (ch=='w' || ch=='x' || ch=='z') aa[1][i][j]='b'; if (ch=='x' || ch=='y' || ch=='z') aa[2][i][j]='c'; } getchar(); } for (int i=1;i&lt;=m;i++) for (int j=1;j&lt;=n;j++) for (int k=0;k&lt;3;k++) { if (aa[k][i][j]=='a'+k) a[k][i][j]=a[k][i-1][j]+1; else a[k][i][j]=0; } for (int k=0;k&lt;3;k++) for (int i=1;i&lt;=m;i++) { int t=0; for (int j=1;j&lt;=n;j++) { while (t&gt;0 &amp;&amp; a[k][i][st[t-1]]&gt;=a[k][i][j]) t--; l[j]=t==0?1:(st[t-1]+1); st[t++]=j; } t=0; for (int j=n;j&gt;=1;j--) { while (t&gt;0 &amp;&amp; a[k][i][st[t-1]]&gt;=a[k][i][j]) t--; r[j]=t==0?n:(st[t-1]-1); st[t++]=j; if (ans&lt;a[k][i][j]*(r[j]-l[j]+1)) ans=a[k][i][j]*(r[j]-l[j]+1); } } printf(\"%d\\n\",ans); } return 0; }","categories":[],"tags":[{"name":"自学","slug":"自学","permalink":"http://yoursite.com/tags/自学/"},{"name":"数据结构","slug":"数据结构","permalink":"http://yoursite.com/tags/数据结构/"}]},{"title":"孤独是你的必修课","slug":"孤独是你的必修课","date":"2016-04-07T02:15:30.000Z","updated":"2016-04-06T13:33:48.000Z","comments":true,"path":"2016/04/07/孤独是你的必修课/","link":"","permalink":"http://yoursite.com/2016/04/07/孤独是你的必修课/","excerpt":"生活不可能像你想象得那么好，但也不会像你想象得那么糟。我觉得人的脆弱和坚强都超乎自己的想象。有时，我可能脆弱得一句话就泪流满面，有时，也发现自己咬着牙走了很长的路。——莫泊桑","text":"生活不可能像你想象得那么好，但也不会像你想象得那么糟。我觉得人的脆弱和坚强都超乎自己的想象。有时，我可能脆弱得一句话就泪流满面，有时，也发现自己咬着牙走了很长的路。——莫泊桑 1但以这样的一句话作为开头，看高木直子的《一个人住第五年》的时候还在国内，那时觉得那样的生活根本不可能发生在我身上，连吃饭都要人陪着的我无法忍受一个人吃饭的感觉。所以后来，有很长的一段时间里我都没能适应一个人吃饭，一个人旅行，现在想想其实也没什么，这个世界运转速度那么快，没有人会在意你是不是一个人。以至于后来一个朋友问我是不是也得了社交恐惧症，我笑笑，其实不是，只是自己慢慢地变得懒了，懒得去经营一份感情，至于朋友，有那么几个就足够了，有些人天天在一起，也不见得是朋友。 好像这样久了，倒是会忘记开始遇到的困难，渐渐地变成自己生活的旁观者，看着生活平静地流淌。都说人是慢慢成长的，其实不是，人是瞬间长大的，就像是突然间沉淀一般，突然不会谈恋爱了或者说不想谈恋爱了，一个人生活单一却也不会觉得无聊，即便很多时候还是会迷茫却也不会觉得烦躁了。 去年的今天我在不一样的城市，背着不一样的书包，留着不一样的发型，走着不一样的路，想着不一样的事情，有着不一样的心思，爱着不一样的人。谁说改变需要十年呢。 2身边的牛人倒是不少，像是神抵一样的存在，我也只是羡慕想着反正自己也不会变成那样的人，直到有一天一个学长跟我聊起来，才知道原来他也有看不进去书的时候也有写论文写到想撞墙的时候，我们都忘了他们是用怎么样的一个代价才换取来了这样的一个人生。他说，如果你想要去实现梦想，孤独是你的必修课。如果不能沉下心来，就没有办法去实现它，因为那绝对不是一件容易的事情，孤独能让你更坚强，你必须找到自己的生活节奏。 有一个朋友喜欢每天喝一点酒，看一部电影然后准时睡觉；住在旁边的英国人神出鬼没有的时候早上才睡有的时候天刚黑就睡了；隔壁楼的一个男生每天天不亮就起来跑步，往往那个时候我才刚打算睡。 最近迷上一个人到处走，算不上旅行只是周围的城市走一遭，倒也不会花上太多时间准备，提起包就走了。我不会带上相机只是有兴致了拿出手机拍一拍，音乐倒是我走到哪里都不能丢的东西，只有音乐，能让看似漫长的等待变成曼妙的旅程，似乎自己跟整个世界都没有关系，只想当一片没有名字的云，徜徉在不知道名字的风景里。 正如上面说的，曾经无法想象一个人吃饭的感觉，同样的，我也不会去想象一个人去坐公交车是什么样的感觉。谁知道没过多久我就习惯了一个人坐车去学校，我离学校比较远所以每次上车的时候还没有多少人，坐最后的几排。有的时候看着窗外发呆，什么都想却又不知道自己在想什么。 我们都会找到自己的生活节奏，然后沉溺其中无法自拔。 3很长一段时间里我都没有去书店，觉得那种“每个星期读一本书”对于我来讲是太遥远的东西。直到有一天我陪朋友去书店，他是一个买书就不会停的人，我也就跟着买了几本。回到家里看微博人人又觉得心里空拉拉的，索性就拿起书来看，也是在那一天我才发现，其实每个星期看一本书没那么难，那天我一下子把书看完，才觉得这样子的生活是充实的。 要么读书，要么旅行，身体和灵魂，必须有一个在路上。 我告诉自己现实容不得你拖延，拖延只会让我变得更焦虑而已，所以刚开始的时候我规定自己每天提早上床半小时，看上几十页书，很快就变成习惯了。有的时候我不得不感叹如果真的去做一件事情的话，那么这件事情没有那么难。当你真的想要做一件事情的时候，整个世界都会来协助你，就是这种感觉。 一个骑过川藏线的朋友说，只要出发，就能到达，你不出发，就哪里也去不了。如果你不能沉下心来，就什么也做不到。出发永远是最有意义的事，去做就是了。一本书买了不看只是几张纸，公开课下了不看也只是一堆数据，不去看就没有任何意义，反而徒增焦虑，行动力才是最关键的。 4你也许也是这样，当你渴望找个人交谈的时候，你们却没有谈什么.于是发现有些事情是不能告诉别人的，有些事情是不必告诉别人的，有些事情是根本没有办法告诉别人的，而有些事情即使告诉了别人，你也会马上后悔。那么最好的办法就是静下来，真正能平静自己的只有自己。 没有人能免得了孤独，与其逃避它不如面对它。孤独并不是一件那么糟糕的事情，与嘈杂相比，一个人生活倒显得自得地多，倒也可以变成一种享受。或许至少需要那么一段时间，几年或几个月，一个人生活，不然怎么能找到自己的节奏知道自己想要什么。这是属于你自己的东西，是你的一部分，你听音乐时，坐地铁时，一个人走在马路上时，它就会流淌出来，让我觉得这个世界似乎在以另外一种形式存在着，我能够清晰地听到自己。 我们都生活在一个不那么如意的世界，当乌云密布我们就摇曳，但阳光总有一天会到来，等阳光照到你的时候，记得开出自己的花就行了，那个你与生俱来的梦想。有的时候梦想很远，有的时候梦想很近，但它总会实现的。我想一个人最好的样子就是平静一点，哪怕一个人生活，穿越一个又一个城市，走过一个又一条街道，仰望一片又一片天空，见证一次又一次别离。 即便世界与我为敌，只要心还透明，就能折射希望。 与你有关的人太多，所以还不如做一个你想要做的人，人生都太短暂，去疯去爱去孤单一场，真正能平静自己的只有自己。人都是孤独的，孤独不可怕，可怕的是惧怕孤独。想要摘星星的孩子，孤独是我们的必修课，我不怕自己努力了不优秀，我只怕比我优秀的人比我更努力。","categories":[],"tags":[{"name":"暖心","slug":"暖心","permalink":"http://yoursite.com/tags/暖心/"},{"name":"转载","slug":"转载","permalink":"http://yoursite.com/tags/转载/"}]},{"title":"机器学习 周志华 第二章模型评估与选择有感","slug":"机器学习 周志华 第二章模型评估与选择有感","date":"2016-04-07T02:15:30.000Z","updated":"2016-03-31T13:17:46.000Z","comments":true,"path":"2016/04/07/机器学习 周志华 第二章模型评估与选择有感/","link":"","permalink":"http://yoursite.com/2016/04/07/机器学习 周志华 第二章模型评估与选择有感/","excerpt":"摘要学这一章之前，我最初的疑问就是：为什么要把这一章放在绪论之后，不先将怎么建立模型呢？后来读着读着才发现只有知道什么样子的模型是好的才能有一个设计模型的方向。这样我们就可以向着这个方向来设计我们的模型了。进入正题：这一章讲的中心就是怎么从多个模型中去选择一个最优的模型。其步骤大致如下：","text":"摘要学这一章之前，我最初的疑问就是：为什么要把这一章放在绪论之后，不先将怎么建立模型呢？后来读着读着才发现只有知道什么样子的模型是好的才能有一个设计模型的方向。这样我们就可以向着这个方向来设计我们的模型了。进入正题：这一章讲的中心就是怎么从多个模型中去选择一个最优的模型。其步骤大致如下： 接着我将对让我比较有感悟的方面做仔细记录。 正文一、首先是经验误差，泛化误差，过拟合，欠拟合等的概念，书上讲的很好很清晰，比老师上课讲的好。 二、然后评估方法里主要是讲了怎么样去在一个总的数据集D中选出（或者说划分为）训练集S与测试集T，才能较好的利用这些数据估计出学习器的自身各种性质。写到这里，我突然想到，估算系统参数，不就是我们概率论中学习的参数估计？我找时间回顾了一下当时学的，主要有两种方法： 1&nbsp;&nbsp;&nbsp;&nbsp; 矩估计法：数据越多，其样本各阶矩会趋近于总体数据的各阶矩。 2&nbsp;&nbsp;&nbsp;&nbsp; 极大似然估计法：参数使得出现被选出样本的情况的概率最大。 三、调参过程书上写的就是对n个互相独立的参数进行枚举取最优，这样的话可以借助神经网络、遗传算法、智能退火等智能算法调参。 四、性能度量，在性能度量的过程中其关键就是怎么样选取一个f(x)使得其可以表示出学习器在我们希望看到的方面的能力。（1）其中最常用的两种是错误率和精度。(若连续则积分)错误率： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; （2-1）精 度： &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; （2-2）（2）到后面在观察其他方面时还会有其他方面的要求倾向，于是就有了查准率P，查全率R。F1是查准率与查全率的加调和平均，而 是T与P的加权调和平均。（调和平均更加注重较小值）。（3）ROC 和AUC是用来看学习器的泛化能力的。与P 和R只是观测的角度不同其余很像。泛化能力： 值得一提的是ROC图的绘制，这个地方的各种式子证明起来都不太好理解，因为一开始不太明白这个图的绘制原理。不过后来就理解了一下。设一共有个正例与个反例，则图像的绘制过程主要是将曲线变成折线而且是只有“向上走”和“向右走”。若出现在这里的是正例则向上走否则向右走（这样正好走到（1,1））。理解方式是：我们希望是所有的正例都在前面，所有的反例都在后面，这样就说明我们学习器学的的经验可以精确的将对错分开。所以在计算&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; （2-3）的时候，理解方式就是没出现一组“反例出现在正例之前”则给1分的罚值，“反例预测值与正例一致”则给0.5分的罚值。其中是每个小格子的面积。 （4）代价敏感错误率与代价曲线，即是真的看成假的与假的看成真的惩罚值不同 五、比较检验：我们希望比较的是泛化性能。通过以上的过程我们已经基本可以比较出不同学习器在测试集上的性能好坏，而我们目前可以通过假设检验来计算把以上结论推到学习器泛化能力的正确概率是多少。（概率论知识，多看书，多理解，不过为什么是那些分布呢？） 六、利用“偏差-方差分解”解释学习算法泛化性能。泛化误差可以分解为偏差、方差与噪声之和。 总结这一章：1 &nbsp;&nbsp;&nbsp;&nbsp; 概率TM原来是这么用的呀。2 &nbsp;&nbsp;&nbsp;&nbsp; 这些观测不同方向给出的公式好巧妙。3 &nbsp;&nbsp;&nbsp;&nbsp; 学习器一定要有一个学习方向！即我们希望他在哪方面性能更好。 问题1. 为什么 P24“若可彻底避免过拟合，则通过经验误差最小化就能获得最优解，这就以为着我们构造性地证明了‘P=NP’；因此，只要相信‘P≠NP’，过拟合就不可避免”？&nbsp;&nbsp;&nbsp;&nbsp;在我看来，这可能意味着我们的方向不是一个单调的线，可能我们的学习器在样本上很好，但是在其他样本上就未必了。仔细想一下，若没有过拟合，则训练误差就是泛化误差，换句话说，在只要被从训练集中总结出的经验就一定是对的，这有没有可能被实现呢？若样本集中可以包含所有特例，且要求学习器必须总结出符合所有样本的经验才采用的话（例如利用学习器求取凸包的话，有没有可能实现“无过拟合”呢？）不过这样的话啊，这确实就是一个P问题，而不是NP问题。过拟合不可避免的前提到底是什么呢？2. 在交叉验证法中的那个特例留一法，其每组模型只有一个测试样本，这样难道不会使其误差过大吗？ 3. P31中第二段第三行“按此顺序逐个把样本作为正例进行预测，则每次可以计算出当前的查全率、查准率”这句话我理解了很多次。目前我认为是一开始让学习器认为所有样本全错，然后在排第一位的对，然后第二个。换句话说，学习器的工作只有排序，而我们的工作是调整学习器的阈值来使在已排好的序列中依次成为正例。所以我们是在调整阈值。不懂的地方是，若两个样本的预测值一样怎么办吗？取平均吗？ 4. 在绘制ROC图时遇到正例预测值与反例预测值一样时，我感觉按照书上的写法是走出来一个三角形，是这样吗？计算AUC与时，若遇到正例预测值与反例预测值一样时，则按小块面积为处理？","categories":[],"tags":[{"name":"machine learning","slug":"machine-learning","permalink":"http://yoursite.com/tags/machine-learning/"},{"name":"自学","slug":"自学","permalink":"http://yoursite.com/tags/自学/"},{"name":"读后感","slug":"读后感","permalink":"http://yoursite.com/tags/读后感/"}]}]}